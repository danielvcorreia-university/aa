{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a239913a-b27f-15e7-8911-53b62bf41c4e"
   },
   "source": [
    "# Lab : Kaggle Credit card fraud detection\n",
    "\n",
    "It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n",
    "\n",
    "The dataset contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "**Objectives:** Compare Logistic Regression classifiers on skewed data. The idea is to compare if preprocessing techniques work better when there is an overwhelming majority class that can disrupt the efficiency of the predictive model. Learn how to apply cross validation (CV) for hyper-parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T16:05:43.790796Z",
     "start_time": "2018-11-03T16:05:43.778100Z"
    },
    "_cell_guid": "029ecde6-086d-7a8e-de44-363a7a23dbd8"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "warnings.filterwarnings('ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore',category=Warning)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b4de5f93-d467-ad7d-4597-03d5f3e89f86"
   },
   "source": [
    "### Load the anonimised dataset\n",
    "\n",
    "Dataset contains only numerical input variables which are the result of a PCA (Principal Component Analysis) transformation. Due to confidentiality issues, the original features and more background information about the data cannot be provided. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount. \n",
    " \n",
    "The last column is the Class:  normal transaction (0),  fraud transaction (1). \n",
    "\n",
    "Load the dataset stored in the file *\"creditcard.csv\"*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:54:12.238585Z",
     "start_time": "2018-11-03T15:54:04.176102Z"
    },
    "_cell_guid": "7e5ca1e3-3597-19d2-b4be-dffd335df630"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set dimensions: (284807, 31)\n",
      "\n",
      "Data set mean: \n",
      "Time      9.481386e+04\n",
      "V1        1.758702e-12\n",
      "V2       -8.252296e-13\n",
      "V3       -9.637438e-13\n",
      "V4        8.316234e-13\n",
      "V5        1.592013e-13\n",
      "V6        4.247308e-13\n",
      "V7       -3.050183e-13\n",
      "V8        8.692882e-14\n",
      "V9       -1.179710e-12\n",
      "V10       7.094921e-13\n",
      "V11       1.875022e-12\n",
      "V12       1.053507e-12\n",
      "V13       7.137575e-13\n",
      "V14      -1.491369e-13\n",
      "V15      -5.225946e-13\n",
      "V16      -2.280687e-13\n",
      "V17      -6.428451e-13\n",
      "V18       4.958999e-13\n",
      "V19       7.060690e-13\n",
      "V20       1.766044e-12\n",
      "V21      -3.406538e-13\n",
      "V22      -5.713357e-13\n",
      "V23      -9.725290e-13\n",
      "V24       1.464144e-12\n",
      "V25      -6.989090e-13\n",
      "V26      -5.615254e-13\n",
      "V27       3.332111e-12\n",
      "V28      -3.518885e-12\n",
      "Amount    8.834962e+01\n",
      "Class     1.727486e-03\n",
      "dtype: float64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"creditcard.csv\", header=0)\n",
    "\n",
    "# Confirm that the dimension of the data set is (284807, 31)    \n",
    "print(\"Data set dimensions: {}\\n\".format(data.shape))\n",
    "\n",
    "#Compute the mean of each column, and observe that the anonimised features V1-V28 \n",
    "#have mean arround 0\n",
    "print(\"Data set mean: \\n{}\\n\".format(np.mean(data)))\n",
    "\n",
    "# show the first few examples (rows) from the dataset \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the values of Column Amount  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10  ...       V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  0.090794  ... -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425 -0.166974  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  0.207643  ...  0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024 -0.054952  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  0.753074  ... -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Class  normAmount  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053      0    0.244964  \n",
       "1  0.167170  0.125895 -0.008983  0.014724      0   -0.342475  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752      0    1.160686  \n",
       "3  0.647376 -0.221929  0.062723  0.061458      0    0.140534  \n",
       "4 -0.206010  0.502292  0.219422  0.215153      0   -0.073403  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#x.reshape(-1, 1) does not mean normalizing between -1,1). \n",
    "#It means collumn vector (-1 is all rows), second dymension is 1.\n",
    "\n",
    "data['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "\n",
    "#drop column Time as irrelevant feature\n",
    "#drop columnt Amount as column normAmount was added\n",
    "data = data.drop(['Time','Amount'],axis=1)  \n",
    "\n",
    "# show again the first few examples (rows) from the normalized dataset \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6268bbd8-6de5-2389-5693-ecd9a14872d4"
   },
   "source": [
    "#### Compute the number of samples per class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 ( fraud transaction): 492\n",
      "Class 0 (normal transaction) : 284315\n"
     ]
    }
   ],
   "source": [
    "number_records_fraud = len(data[data.Class == 1])\n",
    "\n",
    "number_records_normal = len(data[data.Class == 0])\n",
    "\n",
    "print('Class 1 ( fraud transaction):', number_records_fraud)  # ANSWER: Class 1 ( fraud transaction): 492\n",
    "\n",
    "print('Class 0 (normal transaction) :', number_records_normal) # ANSWER: Class 0 (normal transaction) : 284315"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "801dd843-a90b-bb5f-97e7-2da84cc6cd6a"
   },
   "source": [
    "###  Data is totally unbalanced ! How to approach this classification problem:\n",
    "\n",
    "- Collect more data.  Nice strategy but not applicable in this case. \n",
    "- Change the performance metric (do not rely only on the Accuracy): compute Precision, Recall, F1_score.\n",
    "- Resampling the dataset to have an approximate 50-50 ratio:\n",
    "    - By OVER-sampling => add copies of the under-represented class.\n",
    "    - By UNDER-sampling => delete instances from the over-represented class.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cfffc4c5-b621-250f-3b6b-6118cef52b9d"
   },
   "source": [
    "First, extract the features in matrix X and the class labels in vector y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:54:06.954000Z",
     "start_time": "2018-11-03T15:54:06.927158Z"
    },
    "_cell_guid": "c1d874fa-5ea5-edbb-726c-ae98c84e6120"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class\n",
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.iloc[:, data.columns != 'Class']\n",
    "y = data.iloc[:, data.columns == 'Class']\n",
    "\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  UNDER-sampling \n",
    "\n",
    "Apply UNDER-sampling by randomly selecting x samples from the majority class (0), where x is the total number of records with the minority class (1). \n",
    "\n",
    "The under-sampled dataset has a 50/50 class ratio of samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:54:27.473931Z",
     "start_time": "2018-11-03T15:54:27.402424Z"
    },
    "_cell_guid": "2af7c203-44ed-66b6-6141-ac0d0637fcc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of transactions in resampled data: 984\n",
      " # of normal under sample transactions:  492\n",
      " # of fraud transactions:  492\n"
     ]
    }
   ],
   "source": [
    "# Picking the indices of the minority (fraud) class\n",
    "fraud_indices = np.array(data[data.Class == 1].index)\n",
    "\n",
    "# Picking the indices of the normal class\n",
    "normal_indices = np.array(data[data.Class == 0].index)\n",
    "\n",
    "# Number of data points in the minority (fraud) class\n",
    "number_records_fraud = len(fraud_indices)\n",
    "\n",
    "# Out of the normal class indices, randomly select number_records_fraud samples \n",
    "random_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace=False)\n",
    "\n",
    "# Appending the indices of normal and fraud classes\n",
    "under_sample_indices = np.concatenate([fraud_indices, random_normal_indices])\n",
    "\n",
    "# Under sample dataset\n",
    "under_sample_data = data.iloc[under_sample_indices,:]\n",
    "\n",
    "# Balanced under-sampled Data \n",
    "\n",
    "print(\"Total # of transactions in resampled data:\", len(under_sample_data)) #ANSWER:  984\n",
    "print(\" # of normal under sample transactions: \", len(under_sample_data[under_sample_data.Class == 0])) # ANSWER:  492\n",
    "print(\" # of fraud transactions: \", len(under_sample_data[under_sample_data.Class == 1])) # ANSWER:  492\n",
    "\n",
    "#The features in matrix X_undersample, the class labels in vector  y_undersample\n",
    "X_undersample = under_sample_data.iloc[:, under_sample_data.columns != 'Class']\n",
    "y_undersample = under_sample_data.iloc[:, under_sample_data.columns == 'Class']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6b74ba73-82a8-3585-b790-44fe486ff19d"
   },
   "source": [
    "### Explanation of random_state\n",
    "\n",
    "All computers have what is called a pseudo-random number generator. This is something that produces seemingly random numbers, but if kept being repeated, would reproduce the same sequence eventually.\n",
    "Where the number generator is started is known as the seed. When you specify the random_state parameter, you are just setting the random seed for the random number generator.\n",
    "\n",
    "Suppose you set random_seed = 0. The random number generator might then produce the sequence of integers\n",
    "0, 19, 11, 2, 34, 5, 23, 24, 0, 1, 89, â€¦\n",
    "\n",
    "and by fixing random_state=0, you will always see this sequence each time you call your train_test_split function. \n",
    "\n",
    "On the other hand, suppose you set random_state=1 and got the following sequence of integers:\n",
    "91, 18, 11, 34, 34, 5, 19, 18, 0, 0, 1, â€¦\n",
    "\n",
    "You will always see these random numbers when you set random_state = 1. \n",
    "\n",
    "### Train-test data splitting\n",
    "\n",
    "Apply *train_test_split* to the Whole dataset and to the Undersampled dataset with 30% train-test data ratio and random_state = 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:55:48.985351Z",
     "start_time": "2018-11-03T15:55:48.794082Z"
    },
    "_cell_guid": "4a725b16-c14a-2be8-8240-617b7b2ed8cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number transactions train dataset:  199364\n",
      "Number transactions test dataset:  85443\n",
      "Total number of transactions:  284807\n",
      "\n",
      "UNDER-SAMPLED DATA:\n",
      "Number transactions train dataset :  688\n",
      "Number transactions test dataset:  296\n",
      "Total number of transactions:  984\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Whole dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "print(\"Number transactions train dataset: \", X_train.shape[0])   #ANSWER: 199364\n",
    "print(\"Number transactions test dataset: \", X_test.shape[0])    #ANSWER:   85443\n",
    "print(\"Total number of transactions: \", (X_train.shape[0] + X_test.shape[0])) #ANSWER:  284807\n",
    "\n",
    "# Undersampled dataset\n",
    "X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample, y_undersample, test_size = 0.3, random_state = 0)\n",
    "\n",
    "print(\"\\nUNDER-SAMPLED DATA:\")\n",
    "print(\"Number transactions train dataset : \", X_train_undersample.shape[0]) #ANSWER:  688\n",
    "print(\"Number transactions test dataset: \", X_test_undersample.shape[0]) #ANSWER:  296\n",
    "print(\"Total number of transactions: \", (X_train_undersample.shape[0] + X_test_undersample.shape[0])) #ANSWER:  984"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6927cc69-57e6-4f34-4680-0b0016d414a0"
   },
   "source": [
    "###  MODEL 1: Logistic regression classifier - Undersampled data\n",
    "\n",
    "- Accuracy = (TP+TN)/total\n",
    "- Precision = TP/(TP+FP)\n",
    "- Recall = TP/(TP+FN)\n",
    "\n",
    "**Our goal is, do not miss a fraud transaction**, therefore  we are interested in the Recall score, because that is the metric to capture the most fraudulent transactions. Due to the imbalacing of the data, many observations could be predicted as False Negatives, that is, we predict a normal transaction, but it is in fact a fraudulent one. Recall captures this.\n",
    "\n",
    "Precision is less important metric for this problem, because if we predict that a transaction is fraudulent but it is not (this is false positive case), this is not a massive problem compared to the opposite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T00:35:02.476042Z",
     "start_time": "2018-11-04T00:35:02.469768Z"
    },
    "_cell_guid": "9c7ec815-da54-993b-ef8d-b41b767cfacf"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "88765ef8-cb56-860a-d249-9691d90afcb2"
   },
   "source": [
    "### K-fold Cross Validation (CV) to find the best hyper-parameter C of Logistic Regression.  \n",
    "\n",
    "C =1/$\\lambda$, where $\\lambda$ is the regularization parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T00:42:39.453151Z",
     "start_time": "2018-11-04T00:42:39.440375Z"
    },
    "_cell_guid": "069bc837-cfd1-006e-c589-7085d5d29a8e"
   },
   "outputs": [],
   "source": [
    "# Find the best hyper-parameter C. Optimizing for recall perf. metric \n",
    "def print_gridsearch_scores(x_train_data,y_train_data):\n",
    "    c_param_range = [0.01,0.1,1,10]\n",
    "\n",
    "    clf = GridSearchCV(LogisticRegression(), {\"C\": c_param_range}, cv=5, scoring='recall')\n",
    "    clf.fit(x_train_data,y_train_data)\n",
    "\n",
    "    print(\"Best parameters found on CV(dev) set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "\n",
    "    print(\"K-fold Score (Recall) on CV (dev) set:\")\n",
    "    \n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    \n",
    "    #Visualization of the K-fold Recall results for different hyper parameters C\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "        \n",
    "    return clf.best_params_[\"C\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T00:42:40.091983Z",
     "start_time": "2018-11-04T00:42:39.798627Z"
    },
    "_cell_guid": "983c1c75-8092-9a8e-40ca-754fde9e2301"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found on CV(dev) set:\n",
      "\n",
      "{'C': 1}\n",
      "K-fold Score (Recall) on CV (dev) set:\n",
      "0.878 (+/-0.079) for {'C': 0.01}\n",
      "0.896 (+/-0.072) for {'C': 0.1}\n",
      "0.916 (+/-0.072) for {'C': 1}\n",
      "0.916 (+/-0.065) for {'C': 10}\n"
     ]
    }
   ],
   "source": [
    "#Apply function print_gridsearch_scores to get the best C with the Undersampled dataset\n",
    "best_c = print_gridsearch_scores(X_train_undersample, y_train_undersample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e40b554a-5b88-2828-f655-d54560ad7480"
   },
   "source": [
    "### Model 1.1: Logistic Regression trained and tested with undersampled data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T00:42:41.489547Z",
     "start_time": "2018-11-04T00:42:41.241496Z"
    },
    "_cell_guid": "5c8e4c0e-8cfd-7422-04a8-1b47b8531267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (undersample test dataset)\n",
      "[[140   9]\n",
      " [ 11 136]]\n",
      "Recall:  0.9251700680272109\n"
     ]
    }
   ],
   "source": [
    "# Use the best C to train LogReg model with undersampled train data and test it with undersampled test data\n",
    "lr = LogisticRegression(C = best_c)\n",
    "lr.fit(X_train_undersample,y_train_undersample)\n",
    "y_pred_undersample = lr.predict(X_test_undersample)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test_undersample,y_pred_undersample)\n",
    "\n",
    "print('Confusion matrix (undersample test dataset)')\n",
    "print(cnf_matrix)\n",
    "\n",
    "#Compute Recall metric (TP/(TP+FN))\n",
    "confusion_matrix\n",
    "print(\"Recall: \", cnf_matrix[1][1] / (cnf_matrix[1][1] + cnf_matrix[1][0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3aee694e-c838-434a-0cb6-dafaa3a0b224"
   },
   "source": [
    "\n",
    "### Model 1.2: Logistic Regression trained on under-sampled data and tested with the whole test data\n",
    "\n",
    "Apply the same approach as above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T00:42:43.940100Z",
     "start_time": "2018-11-04T00:42:43.645024Z"
    },
    "_cell_guid": "2fac80a6-cc45-49e8-3fd6-2322e2461955"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (whole test dataset)\n",
      "[[81867  3429]\n",
      " [   11   136]]\n",
      "Recall:  0.9251700680272109\n"
     ]
    }
   ],
   "source": [
    "# Use the best C to train LogReg model with undersampled train dataset and test it with whole test dataset\n",
    "lr = LogisticRegression(C = best_c)\n",
    "\n",
    "#train on undersampled data\n",
    "lr.fit(X_train_undersample,y_train_undersample)\n",
    "\n",
    "#predict whole test data \n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Compute and print confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "print('Confusion matrix (whole test dataset)')\n",
    "print(cnf_matrix)\n",
    "\n",
    "# Compute and print Recall metric\n",
    "confusion_matrix\n",
    "print(\"Recall: \", cnf_matrix[1][1] / (cnf_matrix[1][1] + cnf_matrix[1][0])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "29bf4990-1b01-52fc-6c37-d4414b0aafa7"
   },
   "source": [
    "###  ROC curve & AUC\n",
    "\n",
    "Plot the Receiver Operating Characteristic (ROC) curve and compute the Area Under the ROC Curve (AUC). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = LogisticRegression(C = best_c, penalty = 'l2')\n",
    "lr.fit(X_train_undersample,y_train_undersample)\n",
    "y_pred_undersample_score=lr.decision_function(X_test_undersample)\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test_undersample,y_pred_undersample_score)\n",
    "\n",
    "# Compute Area Under the ROC Curve (AUC), it is a scalar \n",
    "roc_auc = auc(fpr,tpr)\n",
    "\n",
    "# Plot ROC\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b',label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.0])\n",
    "plt.ylim([-0.1,1.01])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1f1e40a4-bca8-87ad-f368-d0b50e7d6158"
   },
   "source": [
    "#### REMARK\n",
    "To create the undersampled data, we randomly picked some samples from the majority class. This is a valid technique, however is doesn't represent the real (huge) population. \n",
    "For sufficient statistical credibility, it would be usefull to repeat the process with different undersampled configurations and check if the previous chosen parameters are still the most effective. In the end, the idea is to use a wider random representation of the whole dataset and rely on the averaged best parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2e0e6bfc-37ac-2d2e-61af-7118619fdf27"
   },
   "source": [
    "### MODEL 2: Logistic regression classifier - Skewed data\n",
    "\n",
    "Now, apply K-fold Cross Validation (CV) to find the best hyper-parameter C with whole train data, as it was done above. \n",
    "\n",
    "K-fold is now computationally much more time consuming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T00:43:31.501891Z",
     "start_time": "2018-11-04T00:42:50.753355Z"
    },
    "_cell_guid": "2aaf245f-43cd-d543-b857-562fb696fc4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found on CV(dev) set:\n",
      "\n",
      "{'C': 10}\n",
      "K-fold Score (Recall) on CV (dev) set:\n",
      "0.580 (+/-0.112) for {'C': 0.01}\n",
      "0.609 (+/-0.108) for {'C': 0.1}\n",
      "0.617 (+/-0.114) for {'C': 1}\n",
      "0.620 (+/-0.122) for {'C': 10}\n"
     ]
    }
   ],
   "source": [
    "best_c = print_gridsearch_scores(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the best C to train LogReg model with the whole train data and test it with whole test data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T00:43:40.964169Z",
     "start_time": "2018-11-04T00:43:35.816138Z"
    },
    "_cell_guid": "634c1907-a5c5-888c-c2e9-da73f81ee445"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (whole test dataset)\n",
      "[[85284    12]\n",
      " [   56    91]]\n",
      "Recall:  0.6190476190476191\n"
     ]
    }
   ],
   "source": [
    "# Use the best C to train LogReg model with undersampled train dataset and test it with whole test dataset\n",
    "lr = LogisticRegression(C = best_c)\n",
    "\n",
    "#train on undersampled data\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "#predict whole test data \n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Compute and print confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "print('Confusion matrix (whole test dataset)')\n",
    "print(cnf_matrix)\n",
    "\n",
    "# Compute and print Recall metric\n",
    "confusion_matrix\n",
    "print(\"Recall: \", cnf_matrix[1][1] / (cnf_matrix[1][1] + cnf_matrix[1][0])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 4,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
